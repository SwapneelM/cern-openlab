{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anomaly Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json, csv, sys, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.font_manager\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function extracts the features specified in the array from the json file provided to it and stores it in a csv file at the location provided to it. \n",
    "\n",
    "Edit the file locations (source json; destination csv) and run the code to generate the requisite datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('../dataset/sample_first_50000.json', 'r') as file:\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\t# TODO: change list-based stuff to dict\n",
    "\tdata = {}\n",
    "\tipList = []\n",
    "\t# iterate over each logged line\n",
    "\tfor line in file:\n",
    "\t\tnewItem = {}\n",
    "\t\ttry:\n",
    "\t\t\tjsonData = json.loads(line)\n",
    "\t\texcept:\n",
    "\t\t\tprint \"\\nLine {0} is not in JSON format\".format(i)\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif 'data' in jsonData:\n",
    "\t\t\ttimestamp = str(jsonData['data']['event_timestamp'])\n",
    "\t\t\thour_of_the_day = (float(timestamp[11:13]) + float(timestamp[14:16])/60 + float(timestamp[17:19])/3600)\n",
    "\t\t\t# 0 - Monday; 6 - Sunday\n",
    "\t\t\tday_of_the_week = datetime.datetime(int(timestamp[:4]), \\\n",
    "\t\t\t\tint(timestamp[5:7]), int(timestamp[8:10]), int(timestamp[11:13]), \\\n",
    "\t\t\t\tint(timestamp[14:16]), int(timestamp[17:19])).weekday()\n",
    "\t\t\tnewItem[\"timestamp\"] = timestamp\n",
    "\t\t\tnewItem[\"hour_of_the_day\"] = hour_of_the_day\n",
    "\t\t\tnewItem[\"day_of_the_week\"] = day_of_the_week\n",
    "\t\t\tfeaturesFromData = [\"client_user\", \"client_host\", \"client_ip\", \"client_program\", \"CONNECT_DATA_INSTANCE_NAME\", \"service_name\"]\n",
    "\t\t\tfor feature in featuresFromData:\n",
    "\t\t\t\tif feature in jsonData['data']:\n",
    "\t\t\t\t\tnewItem[feature] = str(jsonData['data'][feature])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnewItem[feature] = \"\"\n",
    "\n",
    "\t\tif 'metadata' in jsonData:\n",
    "\t\t\tfeaturesFromMetadata = [\"timestamp\",\"oracle_sid\", \"hostname\"]\n",
    "\t\t\tfor feature in featuresFromMetadata:\n",
    "\t\t\t\tif feature in jsonData['metadata']:\n",
    "\t\t\t\t\tnewItem[feature] = str(jsonData['metadata'][feature])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnewItem[feature] = \"\"\n",
    "\n",
    "\t\t# ignore cases where data is incomplete/very little to analyse\n",
    "\t\tif len(newItem) <= 2:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tdata[i] = newItem\n",
    "\t\t\tipList.append(newItem[\"client_ip\"])\n",
    "\t\t# increment item number within the data\n",
    "\t\ti += 1\n",
    "\n",
    "if j > 0:\n",
    "\tprint \"Could not store {0} lines due to invalid format\".format(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the data extracted from the json file into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to file.\n"
     ]
    }
   ],
   "source": [
    "fieldNames = ['timestamp', 'hour_of_the_day', 'day_of_the_week', 'client_user', 'client_host', 'client_ip', 'client_program', 'CONNECT_DATA_INSTANCE_NAME', 'service_name', 'oracle_sid', 'hostname']\n",
    "\n",
    "with open('../dataset/preprocessed_first_50000.csv', 'w') as csvFile:\n",
    "\twriter = csv.DictWriter(csvFile, fieldnames=fieldNames)\n",
    "\twriter.writeheader()\n",
    "\t\n",
    "\tfor item in data:\n",
    "\t\twriter.writerow(data[item])\n",
    "print\"Data written to file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data and store it in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1497952189967\n",
      "1    1497952189967\n",
      "2    1497952189967\n",
      "3    1497952189967\n",
      "4    1497952189967\n",
      "Name: timestamp, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../dataset/preprocessed_first_50000.csv').fillna('0')\n",
    "ts = data.loc[:, 'timestamp']\n",
    "data = data.to_dict(orient='records')\n",
    "print ts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DictVectorizer allows us to transform categorical data into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "X = np.array(vec.fit_transform(data).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust Scaling works better for large sparse matrices (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of feature matrix:  (50000, 589)\n"
     ]
    }
   ],
   "source": [
    "X = preprocessing.robust_scale(X)\n",
    "print \"\\nDimensions of feature matrix: \", X.shape\n",
    "np.savetxt('../dataset/matX_ts.txt', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
